# The backend type: terraform
# The environment type: develop, staging or production
backend:
  type: terraform
  environment: develop

# Observatory settings
observatory:
  package: observatory-api
  package_type: pypi
  airflow_fernet_key: {{ airflow_fernet_key }}
  airflow_secret_key: {{ airflow_secret_key }}
  airflow_ui_user_email: my-email@example.com <--
  airflow_ui_user_password: my-password <--
  postgres_password: my-password <--

# Terraform settings
terraform:
  organization: my-terraform-org-name <--

# Google Cloud settings
google_cloud:
  project_id: my-gcp-id <--
  credentials: /path/to/google_application_credentials.json <--
  region: us-west1 <--
  zone: us-west1-a <--
  data_location: us <--

# Google Cloud CloudSQL database settings
cloud_sql_database:
  tier: db-custom-2-7680
  backup_start_time: '23:00'


# Settings for the main VM that runs the Apache Airflow scheduler and webserver
airflow_main_vm:
  machine_type: n2-standard-2
  disk_size: 50
  disk_type: pd-ssd
  create: true

# Settings for the weekly on-demand VM that runs large tasks
airflow_worker_vm:
  machine_type: n1-standard-8
  disk_size: 3000
  disk_type: pd-standard
  create: false

# Elasticsearch
elasticsearch:
  host: https://address.region.gcp.cloud.es.io:port <--
  api_key: API_KEY <--

# API settings
api:
  domain_name: api.observatory.academy <--
  subdomain: project_id

# User defined Apache Airflow variables:
# airflow_variables:
#   my_variable_name: my-variable-value

# User defined Apache Airflow Connections:
# airflow_connections:
#   my_connection: http://my-username:my-password@

# User defined Observatory DAGs projects:
# workflows_projects:
#   - package_name: academic-observatory-workflows
#     package: /path/to/academic-observatory-workflows
#     package_type: editable
#     dags_module: academic_observatory_workflows.dags
