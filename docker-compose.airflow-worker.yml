version: "3.8"
x-airflow-environment: &airflow-env
  AIRFLOW__CORE__EXECUTOR: CeleryExecutor
  AIRFLOW__CORE__SQL_ALCHEMY_CONN: "postgres://airflow:${POSTGRES_PASSWORD}@${POSTGRES_HOSTNAME}:5432/airflow"
  AIRFLOW__CORE__FERNET_KEY: "${FERNET_KEY}"
  AIRFLOW__CORE__AIRFLOW_HOME: "/opt/airflow/"
  AIRFLOW__CORE__LOAD_EXAMPLES: "False"
  AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS: "False"
  AIRFLOW__WEBSERVER__RBAC: "True"
  AIRFLOW__CELERY__BROKER_URL: "redis://:@${REDIS_HOSTNAME}:6379/0"
  AIRFLOW__CELERY__RESULT_BACKEND: "db+postgresql://airflow:${POSTGRES_PASSWORD}@${POSTGRES_HOSTNAME}:5432/airflow"
  AIRFLOW__SECRETS__BACKEND: "airflow.contrib.secrets.gcp_secrets_manager.CloudSecretsManagerBackend"
  AIRFLOW__SECRETS__BACKEND_KWARGS: "{'connections_prefix': 'airflow-connections', 'variables_prefix': 'airflow-variables', 'sep': '-'}"
  GOOGLE_APPLICATION_CREDENTIALS: "/run/secrets/google_application_credentials.json"
  AIRFLOW_VAR_DOWNLOAD_BUCKET_NAME: "download-bucket"
  AIRFLOW_VAR_TRANSFORM_BUCKET_NAME: "transform-bucket"

services:
  worker_remote:
    image: apache/airflow:1.10.10-2-python3.7
    restart: always
    secrets:
      - google_application_credentials.json
    environment:
      <<: *airflow-env
    volumes:
      - "./dags:/opt/airflow/dags"
    command: worker -q remote_queue